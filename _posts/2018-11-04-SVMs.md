---
layout: post
title: "SVMs"
date: 2018-10-05 16:37:00
image: 'https://res.cloudinary.com/dn18ydekv/image/upload/v1541331773/blog_svm/svm_creater.jpg'
description: tutorial on SVMs
category: 'machine learning'
tags:
- machine learning
introduction: tutorial on SVMs
---

> This tutorial focus on kernel-trick and implementation of SVC with Python. Starting with the algorithm itself !

## Optimization & Constraint
*we are not delving deep into those beginners' graphics, but going directly into the Math*

Since we are drawing a hyper-plane that seperates two possible classifications with labeled-dataset,
our quest essentially falls into the optimizing problem .
We exemplfies with the most fundamental genre **Hard margin linear SVC**

### Hard margin linear SVC classifier objective
![purpose](https://res.cloudinary.com/dn18ydekv/image/upload/v1541319721/blog_svm/oc1.png) 

In respect to the **optimization**: we visibly subject to maximizing the 'streets',
which is the largest margin space between two respective sets of support vectors of each
classification.

Accompanied the **Constraint** :We need the decision function to be greate than 1 for all
positive training instances as well as lower than -1 for all negative counterparts.
*Therefore , we expect all instances positive for the hard-margin model*

### Soft margin linear SVC classifier objective
> We introduce soft-margin SVC in case of overfitting scenario
![purpose](https://res.cloudinary.com/dn18ydekv/image/upload/v1541319723/blog_svm/oc2.png)
Most remarkable feature for *soft-margin* SVC is the very famous **C-parameter**  introduced, which allows certaion
extent of mistakes that SVC gonna tolerate .
Moreover , **slack-variable** is also introduced .

### The dual problem

#### Lagrange multipliers
The general idea is to transform a constrained optimization objective into an unconstrained one , 
by moving the *constraints* into the *objective function* . 
**Basically the same idea with Duality**
for e.g. :
```python
optimization: f(x,y) = x^2 + 2*y
constraints:  3x + y + 1 = 0
```
We simply create a new function named *Lagrangian* or *Lagrange function*
```
g(x,y,α) = f(x,y)-α*(3x + y + 1) 
```

However , this method applies only to *equality contraints* . 
Fortunately , under some regularity conditions(which are repected by the SVM objectives) , this method can be 
generalized to *inequlity constraints* as well . (e.g. : 3x+y+1 > 0)
![purpose](https://res.cloudinary.com/dn18ydekv/image/upload/v1541323714/svc_inequal.png)
- where the `α` variables are called the KKT-multiplier , and they must be greater or equal to 0 . 
- t(.) represents the `tranfer function`(e.g. : sigmoid , Gaussian , etc.) 
- the `inequality` must be hold by `equality` : *t(i)((w)'*x(i) + b) = 1*
  this condition its called thed `complementary slackness` , implies that either α(i) = 0
  or the i instance lies on the boundary (a *support vector*)
  
*concrete algorithmic flowchat is as follows :*
![purpose](https://res.cloudinary.com/dn18ydekv/image/upload/v1541325735/blog_svm/primal2dual.png)

finally and hopefully , we get the `dual form` of the SVM problem :
![purpose](https://res.cloudinary.com/dn18ydekv/image/upload/v1541319698/blog_svm/dual_form.png)

### Kernel trick
For non-linear separable scenario , we can actually implements *kernal trick* to project our dataset into a higher
dimensonal space to make in linear-separable .

> Nothing is essentially changed . The only difference is that the dataset is transfered from Rn to Rm (m>n)

Our objective still remains : primal--dual--find α that minimizes the dual-form optimization function--get (w, b) for the hyper-plane
![purpose](https://res.cloudinary.com/dn18ydekv/image/upload/v1541330764/blog_svm/kernel_trick.png)

Here are some commonly used kernels :
![kernels](https://res.cloudinary.com/dn18ydekv/image/upload/v1541319693/blog_svm/common_kernel.png)
(a, b) parallels with (xi, xj) from above .

## Implementation
> first of all discourse , `Jupyter notebook` is absolutely your sublime choice for Python programming
![jupyter](https://res.cloudinary.com/dn18ydekv/image/upload/v1541332216/blog_svm/jupyter.png)

### Import packages for data-visualization & data-analysis & ML
```python
# packages for data-analysis and ML-module
import numpy as np
from sklearn import datasets
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.svm import SVC

# packages for data-visualization
import matplotlib.pyplot as plt
%matplotlib inline
```

### Visualize with spreadsheet or simply numpy within IDE
```python
iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.float64)  # Iris-Virginica
```

### Train two SVC model that differs in C-paramter
As mentioned above , this shall differs in the tolerance of violated samples .
```python
model_svm1 = LinearSVC(C=1, loss="hinge", random_state=42)
model_svm2 = LinearSVC(C=100, loss="hinge", random_state=42)

scaled_svm1 = Pipeline([
    ("scaler", scaler),
    ("model_svm1", model_svm1),
])
scaled_svm2 = Pipeline([
    ("scaler", scaler),
    ("model_svm2", model_svm2),
])

scaled_svm1.fit(X, y)
scaled_svm2.fit(X, y)
```

### Visualize result
![visualize](https://res.cloudinary.com/dn18ydekv/image/upload/v1541333091/different_c.png)

### Kernel trick
As mentioned above , our dataset is visibly non-linear separable , which necessitate us implementing `Kernel trick` !
We use *polynomial* kernel as e.g. 
```python
from sklearn.preprocessing import PolynomialFeatures

"""front paramters are tranformers , while the last shall only be estimator"""
"""Pipeline of transforms with a final estimator."""
model_polynomial_svm = Pipeline([
    ("poly_features", PolynomialFeatures(degree=3)),
    ("scaler", StandardScaler()),
    ("svm_clf", LinearSVC(C=10, loss="hinge", random_state=42))
])

model_polynomial_svm.fit(X, y)
```
BTW , `SVC` has `kernel trick` integrated while `LinearSVC` doesnont .
```python
poly_kernel_svm = Pipeline([
    ("scaler", StandardScaler()),
    ("svm_poly", SVC(kernel="poly", degree=3, coef0=100, C=5))
])
poly_kernel_svm.fit(X, y)
```

### Visualize result
![kernel_trick](https://res.cloudinary.com/dn18ydekv/image/upload/v1541332223/blog_svm/rbf_result.png)

### multi-class classification
`mnist dataset` is a very famous dataset with coprious samples for multi-class which are the 9 *Arabic numerals* .
So it's intrinsically perfect for out multi-class classifying practice . 
![minst](https://res.cloudinary.com/dn18ydekv/image/upload/v1541333652/blog_svm/mnist.png)

#### Import dataset with tensorflow
```python
import tensorflow as tf

mnist = tf.keras.datasets.mnist
(x_train, y_train),(x_test, y_test) = mnist.load_data()
# normalization
x_train, x_test = x_train / 255.0, x_test / 255.0

# representation of labeled data-set
#print(x_train[1])
print(y_train[:10])
print(len(x_train))
print(len(x_test))
```

#### Proportionate `training` & `testing` data
```python
x_train = x_train[:60000]
y_train = y_train[:60000]
x_test = x_test[:10000]
y_test = y_test[:10000]
```

#### Shuffle sequence before model-fitting
```python
# fix the result for every shuffle
np.random.seed(42)
# randomly permutate the sequence of the data-set
rnd_idx = np.random.permutation(60000)
x_train = x_train[rnd_idx]
y_train = y_train[rnd_idx]
```

#### Standardize dataset
Since SVMs are `scaler-sensitive` , we incorporate the `feature scaling` process in case the (vertical/horizontal) scale goes unparalled
with its orthogonal axis . 
```python
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train_reduce)
x_test_scaled = scaler.fit_transform(x_test_reduce)
```

#### Train 3 models
```python
model_svm11 = LinearSVC(C=1, loss="hinge", random_state=42)
model_svm22 = LinearSVC(C=1, loss="hinge", random_state=42)
svm_clf = SVC(kernel="rbf", gamma=5, C=1, decision_function_shape="ovr")

model_svm11.fit(x_train_reduce[:20000], y_train[:20000])
model_svm22.fit(x_train_scaled[:20000], y_train[:20000])
svm_clf.fit(x_train_scaled[:50000], y_train[:50000])

#### OA testing with `accuracy_score`
```python
from sklearn.metrics import accuracy_score
y_pred_linear = model_svm11.predict(x_test_scaled)
y_pred_linear_scaled = model_svm22.predict(x_test_scaled)

print("linear_kernel_OA: {0}\n"
       "linear_kernel_scaled_OA: {1}\n"
       "nonlinear_kernel_OA: {2}"
       .format(accuracy_score(y_test, y_pred_linear), accuracy_score(y_test, y_pred_linear_scaled), accuracy_score(y_test, y_pred_nonlinear)))
```
![result](https://res.cloudinary.com/dn18ydekv/image/upload/v1541335525/blog_svm/comparison.png)
And it's quite visible that the scaled data input achieve a way higher OA that its raw predecessor . 

However , out SVC model with a `rbf` kernel doesnot perform pretty well as theoretically expected . 
Actually , it's ... HIDEOUS....
![result](https://res.cloudinary.com/dn18ydekv/image/upload/v1541335547/blog_svm/nonlinear_result.png)
I'm still brooding what the heck is happening here ...

Finally ,
if you like this post , please star me up in my [Github repo](https://github.com/unclebob7).
Many Thanks !