---
layout: post
title: "Cross Validation"
date: 2018-12-22 19:46:00
image: 'https://raw.githubusercontent.com/unclebob7/SVMs/master/cv_icon.png'
description: tutorial on SVMs
category: 'machine learning'
tags:
- machine learning
introduction: tutorial on cross valication
---

> This tutorial focus on the implementation of cross validation. 

## What is cross validation ?

### Motivation:
Need a way to choose between machine learning models

Goal is to estimate likely performance of a model on out-of-sample data

### Initial idea
Train and test on the same data

But, maximizing training accuracy rewards overly complex models which overfit the training data

### Alternative idea: Train/test split

Split the dataset into two pieces, so that the model can be trained and tested on different data
Testing accuracy is a better estimate than training accuracy of out-of-sample performance
But, it provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy

## Steps for K-fold cross-validation
> Split the dataset into K equal partitions (or "folds").
> Use fold 1 as the testing set and the union of the other folds as the training set.
> Calculate testing accuracy.
> Repeat steps 2 and 3 K times, using a different fold as the testing set each time.
> Use the average testing accuracy as the estimate of out-of-sample accuracy.

## Steps for K-fold cross-validation

### Advantages of cross-validation:

More accurate estimate of out-of-sample accuracy
More "efficient" use of data (every observation is used for both training and testing)

### Advantages of train/test split:

Runs K times faster than K-fold cross-validation
Simpler to examine the detailed results of the testing process

## Cross-validation recommendations

> K can be any number, but K=10 is generally recommended
> For classification problems, stratified sampling is recommended for creating the folds
> Each response class should be represented with equal proportions in each of the K folds
> scikit-learn's cross_val_score function does this by default

## Context
At the end of my previous post, a very much hedious inadequacy was discussed.To recap very quickly : We have only pathetically reached less than 20% of accuracy on the test-set.

## Introspection
And after half month of trials and progressions, I figured out that there're 2 possible elements that could possibly contribute to the low OA rate. Specifically, they're :

> `C-parameter` 
> `gamma` 

The addtional work I have done is to verify that my SVM is a *low-bias*, *high-variance*, which is equally speaking, the sheer size of the training-set(or cross-validation set) shares a proportionate relationship with the *overall accuracy*

## Colab Resorting
A little bit digression before our programming, I'd like to share my ace of absolute computing power, the *COLAB*.

![Colab icon](https://raw.githubusercontent.com/unclebob7/SVMs/master/colab_icon.png)

a hosted Jupyter notebook environment that is free to use and requires no setup. Forceful hardware with 24GB GDDR5 CUDA Cores, No way to let it slide !!!

## Prerequisite 
First, we set up the Runtime type(interpreter) and the hardware accelerator.

![Colab setting](https://raw.githubusercontent.com/unclebob7/SVMs/master/colab_setting.png)

And Bob is youe uncle for the hardware part.

Now we can continue on our usual sequences of ML programming.

## cross validation on C & gamma 
```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal, uniform
from sklearn.model_selection import cross_val_score

param_distributions = {"gamma": reciprocal(0.001, 0.1), "C": uniform(1, 10)}
rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=3, scoring='accuracy')
rnd_search_cv.fit(x_train_scaled[:1000], y_train[:1000])
```

`verbose` : #message to output
`scoring` : metrices for evaluation

Necessary to be mentioned, RandomizedSearchCV implicitly use K-Fold cross-validation(with default K = 3)

![cv gc](https://raw.githubusercontent.com/unclebob7/SVMs/master/cv_gc.png)

RandomizedSearchCV automatically choose the best performed `C-parameter` and `gamma` for our SVM classifier(OA : 85.76%).

## cross validation on number of training set
 ```python
# search for optimal & computationally efficient data size
# from 1000 to 4000 with step size equals to 600
size_range = list(np.linspace(1000, 4000, num=6))

# 列表解析 强制类型转换列表条目为int(automatically set up new list-container)
size_range_int = [int(i) for i in size_range]
CV_scores = []
for size_variable in size_range_int:
    # knn = KNeighborsClassifier(n_neighbors=k)
    
    x_variable = x_train_scaled[:size_variable]
    y_variable = y_train[:size_variable]
    
    scores = cross_val_score(svm_clf, x_variable, y_variable, cv=10, scoring='accuracy')
    CV_scores.append(scores.mean())

print('{:^20} {}'.format('data-size', 'accuracy'))
size_sign = 0;
for score_item in CV_scores:
    print('{:^20} {}'.format(str(size_range_int[size_sign]), score_item))
    size_sign = size_sign+1
 ```

output is as follows :

![output performance](https://raw.githubusercontent.com/unclebob7/SVMs/master/cv_train.png)

Quite visible to discern that the SVM model is a typical *low-bias*, *high-variance* one, with which the increase of training data set along with state-of-art parameter-set, the `OA`(overall accuracy) is likely to rise significantly.
