---
layout: post
title: 'word2vec with Skip-Gram-model'
image: 'https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/skip_gram_model.png'
description: trial of Skip-Gram model for word representation
category: 'NLP'
tags:
- NLP
introduction: tutorial on word2vec
---

> This tutorial focus on the realization of word2vec(Skip-Gram), stemming from Assignment1 of CS224n(Stanford). 

## What is Skip-Gram
From my perspective, It's a terminology for the process of which we **train the weights of the single hidden unit in the Skip-Garm neural-network, and see these weights as the actual word-vectors**, which is technically called `representation of the context words`.

So when the training of this network is finished, the usage is as follows:
* pass in a specific word in the database as input 
* receive a vector telling us the probability for every word in our vocabulary of being the `nearby word` that we choose.(When we say nearby, there's a specific radius for this center-wors, typically 5)

i.e.
> The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word *Soviet*, the output probabilities are going to be much higher for words like *Union* and *Russia* than for unrelated words like *watermelon* and *kangaroo*.The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”.

Skip-Gram illustration:
![Skip-Gram illustration](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/1607485737.jpg)

neural-network model representation:
![model representation](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/1834814060.jpg)


## Cost and Gradient (Softmax cost function)
cost function(softmax) of SGD(stochastic gradient descent) is as follows:
![cost function](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/Screenshot%20from%202019-01-31%2021-36-03.png)

respectice gradient for `center-word` and `context-word` :

nabla Vc:
![nabla Vc](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/Screenshot%20from%202019-01-31%2021-40-16.png)

nabla Uo:
![nabla Uo](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/Screenshot%20from%202019-01-31%2021-15-05.png)

```python
def softmaxCostAndGradient(predicted, target, outputVectors, dataset):
    """ Softmax cost function for word2vec models
    Implement the cost and gradients for one predicted word vector
    and one target word vector as a building block for word2vec
    models, assuming the softmax prediction function and cross
    entropy loss.
    Arguments:
    predicted -- numpy ndarray, predicted word vector (\hat{v} in
                 the written component)
    target -- integer, the index of the target word
    outputVectors -- "output" vectors (as rows) for all tokens
    dataset -- needed for negative sampling, unused here.
    Return:
    cost -- cross entropy cost for the softmax word prediction
    gradPred -- the gradient with respect to the predicted word
           vector
    grad -- the gradient with respect to all the other word
           vectors
    We will not provide starter code for this function, but feel
    free to reference the code you previously wrote for this
    assignment!
    """

    N, V = outputVectors.shape
    y_ = softmax(outputVectors.dot(predicted))
    cost = np.log(np.sum(np.exp(outputVectors.dot(predicted)), axis=0)) - outputVectors[target].dot(predicted)


    gradPred = np.sum(y_.reshape(-1,1)*outputVectors, axis = 0) - outputVectors[target]


    grad = y_.reshape(-1,1)*np.tile(predicted, (N,1))           # 非标注函数关系，均为数乘
    grad[target] -= predicted



    ### END YOUR CODE
    assert gradPred.shape == predicted.shape
    assert grad.shape == outputVectors.shape

    return cost, gradPred, grad
```

## Skip-Gram (word2vec model)

```python
def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,
             dataset, word2vecCostAndGradient=softmaxCostAndGradient):
    """ Skip-gram model in word2vec
    Implement the skip-gram model in this function.
    Arguments:
    currrentWord -- a string of the current center word
    C -- integer, context size
    contextWords -- list of no more than 2*C strings, the context words
    tokens -- a dictionary that maps words to their indices in
              the word vector list
    inputVectors -- "input" word vectors (as rows) for all tokens
    outputVectors -- "output" word vectors (as rows) for all tokens
    word2vecCostAndGradient -- the cost and gradient function for
                               a prediction vector given the target
                               word vectors, could be one of the two
                               cost functions you implemented above.
    Return:
    cost -- the cost function value for the skip-gram model
    grad -- the gradient with respect to the word vectors
    """

    cost = 0.0
    gradIn = np.zeros(inputVectors.shape)
    gradOut = np.zeros(outputVectors.shape)

    ### YOUR CODE HERE
    j = tokens[currentWord]
    predicted = inputVectors[j]
    for word in contextWords:       # list of no more than 2*C strings
        target = tokens[word]
        cost_, gradPred, grad = word2vecCostAndGradient(predicted, target, outputVectors, dataset)

        cost += cost_
        gradIn[j] += gradPred
        gradOut += grad

    ### END YOUR CODE

    return cost, gradIn, gradOut
```

## result
after training with SGD, the result is as follows:
![result](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/q3_word_vectors.png)

## related links
[Skip-Gram tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
[CS224n assigment1](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/assignment1/assignment1.pdf)