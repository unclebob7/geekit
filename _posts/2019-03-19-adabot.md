---
layout: post
title: 'Adabot'
image: 'https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/adabot.jpeg'
description: FEC in Python and its practition in ASR
category: 'program synthesis'
tags:
- program synthesis
introduction: tentative prototype on program synthesis
---

> For hundreds of years, we human has been questing for a way to program in natural language. 

## Retrospection in the history of programming

- 1843-----Ada Lovelace----the world's first published computer program
- 1949----John Mauchly----one of the first high-level languages ever developed for an electronic computer
- 1950s----The first programming language designed to communicate instructions to a computer
- 1954----Fortran----first widely used high level general purpose programming language to have a functional implementation
- 1980---C++
- 1984---MATLAB

## Programming mood
- imperative programming

![imperative programming](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/imperative_programming.png)

- declarative programming

![declarative programming](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/declarative_programming.png)

## Trials

### DeepCoder(2017.03)

First introduced by Microsoft in 2017.03, attempting to initialize a revolution in the field of PS, based PS on the mode of DSL+search. Pretty innovative, but result still remained primitive and premature.

![deepcoder model](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/deepcoder_model.png)

Given pairs of arbitrary number of I/O sets, DeepCoder used a DNN to encode programs within database with Domain Specific Language(DSL). Then decode the DSL-encoded vector by multiplying a learned-matrix. Finally, based the searching on the output multi-hot vector.

Intuitive thinking pattern of Deepcoder is as follow : 

![deepcoder](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/deepcoder.png)

Visibly, DeepCoder might be useful for a **Callable**, function(given I/O parameters), but not necessarily for a common **Runnable** function(without I/O parameters).

### TRANX(2018.10)

![tranx model](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/tranx_model.png)

Introduced by CMU, The ace of TRANX is the introduction of traditional **syntactic parsing** method for PS. 

Sequences are as follows : 
- Specify demands with Input Utterance
- Build Abstract Syntax Tree(AST) with a transition system.
- decode AST into Meaning Representation(MR) with AST_TO_MR() function

From my personal viewpoint, TRANX has implemented a way more intuitive and deeper excavation of the understanding of "programming".

Namely, a bionic simmulation of "human programmer".
- Crystalize our programming purpose of programming with Natural Language.

## Introspection

> How On Earth Do We Human Program ?

![how do we program](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/the_way_we_program.png)

Given question above, the following is what we commonly do.

Human : I gotta an 'object' that first sort the n array, then take the k smallest elements, and finally sum them up. **Specifically**, for example, given a set of [4,3,6,7,1] and 2 people, it wanna get [1,3]

Ultimately intuitive, but unconsciously we combine the 2 worlds together(**DeepCoder** & **TRANX**).

- Specify our purpose with **NL utterance**
- instantialize out purpose with actual I/O

## Adabot(2019.03)

All right, we now first introduce some tentaive alterations in **TRANX** and combine it wth **DeepCoder**. Just to remind, Adabot is still at the stage of design.

![adabot model](https://raw.githubusercontent.com/unclebob7/geekit/gh-pages/assets/img/adabot_model.png)

encoder sequences are as follows :

- Use Named-Entity Recognition(NER) to extract elements out of NL-utterance
- Parse NL-utterance in dependency-parsing manner into a parse-tree
- hightlight the elements extracted with NER in the parse-tree
- Use Viterbi algorithm to backtrack the paths
- Encode each path
- AST_to_MR()

Our probability model is `p(β|θ)` : 

- `β` : encoded shortest paths of our purpose(description) parse tree
- `θ` : NL purpose(utterance)


The combination still remains to be specified...











